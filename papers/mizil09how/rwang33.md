Overview:

In this paper, the authors try to understand and model how opinions are evaluated within online communities. For example, on Amazon.com, the users not only write product reviews, but also provide us with an indication of the helpfulness of the reviews ("26 of 32 people found the following review helpful"). Previous work have shown that helpfulness votes of reviews on Amazon.com are not necessarily strongly correlated with certain measures of review quality. Rather, various complex social feedback mechanisms tend to affect how Amazon users evaluate each others' reviews in practice.

Algorithm:



Hypothesis:

The conformity hypothesis: 

a review is evaluated as helpful when its star rating is closer to the consensus (or average) star rating for the product.

The individual-bias hypothesis: 

a user will rate a review more highly if it expresses an opinion that he or she agrees with. Notice that if a diverse range of users apply this rule, then the overall helpfulness evaluation would be hard to distinguish from one based on conformity.

The brilliant-but-cruel hypothesis: 

negative reviewers are perceived as more intelligent, competent and expert than positive reviewers.

The quality-only straw-man hypothesis: 

the helpfulness of the review is being evaluated, purely based on the textual content of the reviews.
